{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.interpolate as interp\n",
    "import astropy.constants as const\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import emcee\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Note!}$ The folder containing this notebook should be in the same directory as the folder containing the ``custom-complete`` tool for calculating Kepler survey completeness in bins of stellar properties. You can download that tool [here](https://github.com/gbergsten/custom-completeness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../Completeness/')\n",
    "from completeness import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Shorthand for returning period and radius limits\n",
    "def GetRange():\n",
    "    Pmin, Pmax = 2, 100\n",
    "    Rmin, Rmax = 1.0, 3.5\n",
    "\n",
    "    return Pmin, Pmax, Rmin, Rmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Handy shorthand function for making log-spaced grids to integrate with\n",
    "def log_grid(nx=100,ny=100):\n",
    "    Pmin,Pmax,Rmin,Rmax = GetRange()\n",
    "    \n",
    "    x= np.geomspace(Pmin,Pmax,nx)\n",
    "    y= np.geomspace(Rmin,Rmax,ny)\n",
    "    P,R= np.meshgrid(x,y)\n",
    "\n",
    "    logspace_x = np.exp((np.log(x[nx-1])-np.log(x[0]))/(nx-1))\n",
    "    logspace_y = np.exp((np.log(y[ny-1])-np.log(y[0]))/(ny-1))\n",
    "    dP,dR = P*(logspace_x-1), R*(logspace_y-1)\n",
    "    \n",
    "    return P,R,dP,dR,x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# A way to scale the radius valley based on host star mass (using scaling from Wu 2019).\n",
    "\n",
    "def ScaledRadius(R=2, M=1, inverse=False):\n",
    "    ### Radius given in Earth radii\n",
    "    ### Mass given in solar masses\n",
    "    \n",
    "    power = 1/4\n",
    "    if inverse:\n",
    "        power = -power\n",
    "\n",
    "    Rprime = R * (M)**(power)\n",
    "    return Rprime\n",
    "\n",
    "Rsplit = ScaledRadius()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLDF: dF / dRdP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Shape Function (describes behavior of planet population)\n",
    "\n",
    "### Free parameter arrangement:\n",
    "### F0: Avg. # of planets per star\n",
    "### P_break: Break in period for power law\n",
    "### beta_1: Exponent prior to P_break\n",
    "### beta_2: Exponent after P_break\n",
    "### P_central: Center of hyperbolic tangent between sE dominance and sN dominance\n",
    "### chi_1: Ratio of sE to sN prior to P_central\n",
    "### chi_2: Ratio of sE to sN after P_central\n",
    "### s: smoothness of tanh\n",
    "\n",
    "def g(P,R,fparam):\n",
    "        \n",
    "    F0, P_break, beta_1, beta_2, P_central, chi_1, chi_2, s = fparam\n",
    "    \n",
    "    # Use a broken power law to calculate overall occurrence\n",
    "    occ_SEMN = (P/P_break)**np.where(P<P_break, beta_1, beta_2)\n",
    "    \n",
    "    # Use a hyperbolic tangent which changes with period\n",
    "    sx = 0.5 + 0.5*np.tanh((np.log10(P)-np.log10(P_central))/np.log10(s))\n",
    "    # Normalize sides of the tanh curve to chi_1 and chi_2\n",
    "    X = chi_1*(1-sx) + sx*(chi_2)\n",
    "    \n",
    "    # Some normalization to incorporate this form into the shape function\n",
    "    a,b,c = np.log(Rmin), np.log(Rsplit), np.log(Rmax)\n",
    "    g2 = X*(c-b) / ((b-a) + X*(c-b) - X*(b-a))\n",
    "    \n",
    "    # Provide g2 for super-Earths, the complement 1-g2 for sub-Neptunes\n",
    "    g_split = np.where(R<Rsplit, g2, 1-g2)\n",
    "        \n",
    "    # Divide the broken power law's occurrence amongst the super-Earths and/or sub-Neptunes.\n",
    "    g = g_split/R * occ_SEMN \n",
    "        \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "intP,intR,dP,dR,x,y = log_grid()\n",
    "\n",
    "# Normalization parameter for the PLDF (as in Burke et al. 2015).\n",
    "# Ensures that the product of Cn * g equals to 1 when integrated over the P,R range used.\n",
    "def function_Cn(fparam):\n",
    "    \n",
    "    Cn = 1 / np.sum(g(intP,intR,fparam) * dP * dR)\n",
    "    \n",
    "    return Cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# The full PLDF to give differential occurrence rates, defined as df / (dP dR) \n",
    "# When integrated over the P,R range used, equals the number of planets per star F0.\n",
    "\n",
    "def df(P,R,fparam):\n",
    "        \n",
    "    F0 = fparam[0]\n",
    "    Cn = function_Cn(fparam)\n",
    "    \n",
    "    df = F0 * Cn * g(P,R,fparam)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model Likelihood\n",
    "Following the equations of Burke et al. (2015) and Youdin (2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate N_exp (the predicted number of detections), which involves scaling the shape function $g$ by the survey completeness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def function_Nexp(fparam):\n",
    "    \n",
    "    F0 = fparam[0]\n",
    "    Cn = function_Cn(fparam)\n",
    "\n",
    "    integrand = OneRunCompletenessGrid * g(intP,intR,fparam) * dP * dR\n",
    "        \n",
    "    integral = np.sum(OneRunCompletenessGrid * g(intP,intR,fparam) * dP * dR)\n",
    "    \n",
    "    Nexp = F0 * Cn * integral\n",
    "    return Nexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorthand for returning the number of planets being fit (i.e., length of the P and R data arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def function_Npl(P,R):\n",
    "    assert len(P) == len(R)\n",
    "    Npl = len(P)\n",
    "    return Npl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the log likelihood of the given parameters to match the data through the PLDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### NOTE: Here fparam is passed FIRST, rather than last.\n",
    "### This is a workaround for the order of args scipy.minimze takes.\n",
    "\n",
    "def lnL(fparam,P,R):\n",
    "    Npl = function_Npl(P,R)\n",
    "    Nexp = function_Nexp(fparam)\n",
    "    \n",
    "    F0 = fparam[0]\n",
    "    Cn = function_Cn(fparam)\n",
    "    \n",
    "    g_term = np.sum(np.log(g(P,R,fparam)))\n",
    "    \n",
    "    lnL = -Nexp + Npl*np.log(F0) + Npl*np.log(Cn) + g_term\n",
    "    \n",
    "    return lnL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part mostly just follows the emcee documentation.\n",
    "As such, it's likely where things can go wrong.\n",
    "https://emcee.readthedocs.io/en/stable/tutorials/line/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(fparam):\n",
    "    \n",
    "    F0, P_break, beta_1, beta_2, P_cent, chi_1, chi_2, s = fparam\n",
    "        \n",
    "    # Priors are generally uniform with ranges motivated by the data.\n",
    "    # See Appendix C.2 of Bergsten et al. (2022) for more details.\n",
    "    \n",
    "    if \\\n",
    "    0.05 < F0 < 5 and\\\n",
    "    2< P_break < 20 and\\\n",
    "    -2 < beta_1 < 5 and\\\n",
    "    -2 < beta_2 < 5 and\\\n",
    "    4 < P_cent < 50 and\\\n",
    "    (1-chi_1) < chi_1 and\\\n",
    "    0 < chi_1 < 1 and\\\n",
    "    0 < chi_2 < 0.5 and\\\n",
    "    1 < s < 5 and\\\n",
    "    np.log10(2*Pmin) <= np.log10(P_cent) - np.log10(s) and\\\n",
    "    np.log10(0.5*Pmax) >= np.log10(P_cent) + np.log10(s):\n",
    "        return 0.0\n",
    "    \n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Combine with the log likelihood to create the log posterior\n",
    "\n",
    "### NOTE: Here fparam is passed FIRST, rather than last.\n",
    "### This is a workaround for the order of args emcee takes.\n",
    "\n",
    "def log_posterior(fparam,P,R):\n",
    "    \n",
    "    lp = log_prior(fparam)\n",
    "    \n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    \n",
    "    return lp + lnL(fparam,P,R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Data/Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Kepler Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a subset of dwarf stars in the desired mass range.\n",
    "def get_mass_range(stars, row):\n",
    "    # Apply a cut to select only dwarf stars (in the style of Huber et al.)\n",
    "    isdwarf= stars['logg'] > 1./4.671 * np.arctan((stars['Teff']-6300.)/-67.172)+ 3.876\n",
    "    inBin = (stars['Mass'] >= row['Lo']) & (stars['Mass'] < row['Hi']) & isdwarf\n",
    "    return stars[inBin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: It is strongly recommended that column names contain only upper and lower-case ASCII letters, digits, or underscores for maximum compatibility with other software (got '[Fe/H]'). [astropy.io.fits.column]\n"
     ]
    }
   ],
   "source": [
    "### Reading in the Berger+2020a Gaia-Keppler stellar properties catalogue\n",
    "gaia= Table.read('../Completeness/files/Berger2020a_GaiaKeplerCatalog_Table2.fits')\n",
    "### Load in the set of stars with detection metrics (courtesy of Gijs)\n",
    "metrics_list = np.load('../Completeness/files/metricslist.npz')\n",
    "### Find the subset of stars with both detection metrics and Gaia data\n",
    "gaia_kepler_subset = np.intersect1d(gaia['KIC'], metrics_list['KID'])\n",
    "goodstars = np.isin(gaia['KIC'], gaia_kepler_subset)\n",
    "kg = gaia[goodstars]\n",
    "\n",
    "### Cut to select only dwarf stars (following Huber et al. 2016)\n",
    "isdwarf = (kg['logg'] > 1./4.671 * np.arctan((kg['Teff']-6300.)/-67.172)+ 3.876)\n",
    "kg = kg[isdwarf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Info about Mass Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file with info about ranges for equally populated bins of stellar mass\n",
    "mbfilename = 'MassBins-EqualCounts_6bins.txt'\n",
    "t = Table.read('files/{}'.format(mbfilename),\\\n",
    "                        format='csv', names=['Lo', 'Hi'], data_start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in planet properties catalog, including per-candidate reliability\n",
    "KOIcatalog = pd.read_csv('files/dr25_KOIs_dr25.csv')\n",
    "reliability = KOIcatalog.totalReliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the PLDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initial guesses to help with fitting\n",
    "guess = [[0.680, 9.223, 0.639, -0.868, 11.419, 0.847, 0.395, 2.496],\n",
    "         [0.952, 13.224, 0.220, -1.121, 7.555, 0.756, 0.342, 1.505],\n",
    "         [0.742, 5.764, 1.199, -0.663, 11.013, 0.739, 0.375, 1.977],\n",
    "         [0.667, 7.006, 0.907, -0.856, 13.205, 0.826, 0.278, 2.524],\n",
    "         [0.656, 12.123, 0.404, -1.072, 16.804, 0.838, 0.327, 2.262],\n",
    "         [0.537, 7.144, 1.833, -0.730, 17.025, 0.879, 0.388, 2.147]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "intP,intR,dP,dR,x,y = log_grid()\n",
    "Pmin,Pmax,Rmin,Rmax = GetRange()\n",
    "\n",
    "num_draws = 100\n",
    "nstep = 10000\n",
    "\n",
    "for this_row in range(len(t)):\n",
    "    \n",
    "    # Find which mass bin we're working in\n",
    "    row = t[this_row]\n",
    "    # Grab initial guesses for that bin\n",
    "    initial=guess[this_row]\n",
    "    \n",
    "    ### Initialization of all walkers\n",
    "    nwalk = 16\n",
    "    np.random.seed(42)\n",
    "    npos = nwalk\n",
    "    pos = initial + 1e-4 * np.random.randn(nwalk, len(initial))\n",
    "    \n",
    "    #####################################\n",
    "    \n",
    "    thisString = str(row['Lo']) + '-' + str(row['Hi'])\n",
    "    \n",
    "    tracker = []\n",
    "    unused = []\n",
    "    \n",
    "    # Middle of the mass bin\n",
    "    Mavg = np.mean([row['Lo'], row['Hi']])\n",
    "    \n",
    "    # Calculate the average survey completeness for this bin of stellar mass\n",
    "    Pgrid, Rgrid, completeness, nstars = get_completeness(ranges={'Mass':[row['Lo'],row['Hi']]})\n",
    "    # Make an interpolation for the \"total\" survey completeness, approximated by (avg * nstars)\n",
    "    SummedEtaInterp = interp.interp2d(Pgrid,Rgrid,completeness.T*nstars) \n",
    "    OneRunCompletenessGrid = SummedEtaInterp(x,y)\n",
    "    \n",
    "    #####################################\n",
    "    \n",
    "    ### Adjusting star and planet samples\n",
    "    ### Mass cut.\n",
    "    kg2 = get_mass_range(kg, row)\n",
    "    \n",
    "    ### Load up the Kepler dr25 survey\n",
    "    planets_path = 'files/q1_q17_dr25_koi.tbl'\n",
    "    koi = Table.read(planets_path, format='ipac')\n",
    "    iscandidate= koi['koi_pdisposition']=='CANDIDATE'\n",
    "    isreliable= koi['koi_score']>= 0.0 \n",
    "\n",
    "    isGood = np.isin(koi['kepid'], kg2['KIC']) & iscandidate #& isreliable\n",
    "\n",
    "    ### Get rid of the 3(?) planets where there's no transit depth info\n",
    "    ### (required for the step of updating planet radii)\n",
    "    isUseless = koi[isGood]['koi_ror'].mask\n",
    "    print('There are {} planets we cant use bc they lack transit depth.'.format(np.sum(isUseless)))\n",
    "    koi = koi[isGood][~isUseless]\n",
    "\n",
    "    Kepler_P = koi['koi_period']\n",
    "    Kepler_R = koi['koi_prad']\n",
    "\n",
    "    ### Updating the planet radii based on Berger stellar radii\n",
    "    updated_radii = []\n",
    "    ### Also store the reliability to draw from later\n",
    "    sample_reliability = []\n",
    "    for p in koi:\n",
    "        updated_radii.append((p['koi_ror']*kg[np.where(kg['KIC']==p['kepid'])]['Rad'].data*const.R_sun/const.R_earth).value[0])\n",
    "        sample_reliability.append(KOIcatalog.iloc[np.where(KOIcatalog.kepoi_name==p['kepoi_name'])].totalReliability.values[0])\n",
    "    Kepler_R = updated_radii\n",
    "    \n",
    "    ### Limiting the Kepler Sample to the desired P,R range.\n",
    "    Kepler_P, Kepler_R = np.asarray(Kepler_P), np.asarray(Kepler_R)\n",
    "    inRange = np.where((Pmin < Kepler_P) & (Pmax > Kepler_P) & (Rmin < Kepler_R) & (Rmax > Kepler_R))\n",
    "    Kepler_P = Kepler_P[inRange]\n",
    "    Kepler_R = Kepler_R[inRange]\n",
    "    sample_reliability = np.array(sample_reliability)[inRange]\n",
    "    \n",
    "    ### Fully exclude things with zero reliability (typically the 100% false positives)\n",
    "    isnonzeroReliability = (sample_reliability != 0)\n",
    "    Kepler_P = Kepler_P[isnonzeroReliability]\n",
    "    Kepler_R = Kepler_R[isnonzeroReliability]\n",
    "    sample_reliability = sample_reliability[isnonzeroReliability]\n",
    "    \n",
    "    print('{} M: {} stars; {} planets'.format(thisString,len(kg2),len(Kepler_P)))\n",
    "        \n",
    "    #####################################\n",
    "    \n",
    "    ### Adjust the boundary between sE and mN by bin mass\n",
    "    Mavg = np.mean([row['Lo'], row['Hi']])\n",
    "    Rsplit = ScaledRadius(M=Mavg)\n",
    "    print('The boundary between sE and mN is at {:.2f} Re.'.format(Rsplit))\n",
    "    \n",
    "    #####################################\n",
    "    \n",
    "    # Performing sample draws based on candidate reliability\n",
    "    use_array = np.array([np.random.choice([True,False], size=num_draws, p=[rel, 1-rel]) for rel in sample_reliability])\n",
    "\n",
    "    # Identifying any unused planets (things that have low (~<1%) reliability)\n",
    "    # Sometimes a very-low-reliability candidate is never incorporated, \n",
    "    # we opted to let the reliability speak for itself (i.e., we don't do anything with the ignored ones)\n",
    "    if np.any([np.all(use_array[i,:]==False) for i in range(len(sample_reliability))]):\n",
    "        print('{} planets were never used!'.format(len(np.where([np.all(use_array[i,:]==False) for i in range(len(sample_reliability))])[0])))\n",
    "        unused = np.where([np.all(use_array[i,:]==False) for i in range(len(sample_reliability))])[0]\n",
    "    print('Unused Indices:', unused)\n",
    "    \n",
    "    ### MCMC Steps\n",
    "    nwalkers, ndim = pos.shape\n",
    "    timestr = time.strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "     \n",
    "    # Storing info about the unused candidates just for bookkeeping\n",
    "    if len(unused) != 0:\n",
    "        print('Reliability of unused candidates:', sample_reliability[unused])\n",
    "        np.savetxt('files/MCMC_Outputs/unused_flag_{}M_v'.format(thisString.replace('.','p')) + timestr + \".csv\",\n",
    "                   unused, delimiter=',')\n",
    "    \n",
    "    # Filename for storing MCMC results - indexed by start time.\n",
    "    filename = \"files/MCMC_Outputs/Fit_{}M_v\".format(thisString.replace('.','p')) + timestr \n",
    "    print(filename)\n",
    "    \n",
    "    for draw in range(num_draws):\n",
    "        this_Kepler_P = Kepler_P[use_array[:,draw]]\n",
    "        this_Kepler_R = Kepler_R[use_array[:,draw]]\n",
    "        tracker.append([draw,len(this_Kepler_P)])\n",
    "        \n",
    "        this_draw_filename = filename + '_draw_' + str(draw).zfill(3) + \".h5\"\n",
    "        \n",
    "        backend = emcee.backends.HDFBackend(this_draw_filename)\n",
    "        backend.reset(nwalkers, ndim)\n",
    "        with Pool() as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior,\n",
    "                                            args=(this_Kepler_P, this_Kepler_R),\n",
    "                                            backend=backend, pool=pool)\n",
    "            sampler.run_mcmc(pos, nstep, progress=True)\n",
    "        \n",
    "    np.savetxt('files/MCMC_Outputs/tracker_{}M_v'.format(thisString.replace('.','p')) + timestr + \".csv\",\n",
    "                   tracker, delimiter=',')\n",
    "    \n",
    "    print('Runs Completed.')\n",
    "    print()\n",
    "    print('---')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
